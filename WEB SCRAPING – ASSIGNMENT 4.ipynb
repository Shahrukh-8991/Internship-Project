{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471c4386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rank': '\"Baby Shark Dance\"[7]', 'Name': \"Pinkfong Baby Shark - Kids' Songs & Stories\", 'Artist': '14.32', 'Upload Date': '[A]', 'Views': 'June 17, 2016'}\n",
      "{'Rank': '\"Despacito\"[10]', 'Name': 'Luis Fonsi', 'Artist': '8.41', 'Upload Date': '[B]', 'Views': 'January 12, 2017'}\n",
      "{'Rank': '\"Johny Johny Yes Papa\"[18]', 'Name': \"LooLoo Kids - Nursery Rhymes and Children's Songs\", 'Artist': '6.89', 'Upload Date': '', 'Views': 'October 8, 2016'}\n",
      "{'Rank': '\"Bath Song\"[19]', 'Name': 'Cocomelon - Nursery Rhymes', 'Artist': '6.66', 'Upload Date': '', 'Views': 'May 2, 2018'}\n",
      "{'Rank': '\"Shape of You\"[20]', 'Name': 'Ed Sheeran', 'Artist': '6.23', 'Upload Date': '[C]', 'Views': 'January 30, 2017'}\n",
      "{'Rank': '\"See You Again\"[23]', 'Name': 'Wiz Khalifa', 'Artist': '6.22', 'Upload Date': '[D]', 'Views': 'April 6, 2015'}\n",
      "{'Rank': '\"Wheels on the Bus\"[28]', 'Name': 'Cocomelon - Nursery Rhymes', 'Artist': '6.01', 'Upload Date': '', 'Views': 'May 24, 2018'}\n",
      "{'Rank': '\"Phonics Song with Two Words\"[29]', 'Name': 'ChuChu TV Nursery Rhymes & Kids Songs', 'Artist': '5.75', 'Upload Date': '', 'Views': 'March 6, 2014'}\n",
      "{'Rank': '\"Uptown Funk\"[30]', 'Name': 'Mark Ronson', 'Artist': '5.18', 'Upload Date': '', 'Views': 'November 19, 2014'}\n",
      "{'Rank': '\"Gangnam Style\"[31]', 'Name': 'Psy', 'Artist': '5.10', 'Upload Date': '[E]', 'Views': 'July 15, 2012'}\n",
      "{'Rank': '\"Learning Colors – Colorful Eggs on a Farm\"[36]', 'Name': 'Miroshka TV', 'Artist': '5.09', 'Upload Date': '', 'Views': 'February 27, 2018'}\n",
      "{'Rank': '\"Dame Tu Cosita\"[37]', 'Name': 'Ultra Records', 'Artist': '4.59', 'Upload Date': '', 'Views': 'April 5, 2018'}\n",
      "{'Rank': '\"Masha and the Bear – Recipe for Disaster\"[38]', 'Name': 'Get Movies', 'Artist': '4.57', 'Upload Date': '', 'Views': 'January 31, 2012'}\n",
      "{'Rank': '\"Axel F\"[39]', 'Name': 'Crazy Frog', 'Artist': '4.45', 'Upload Date': '', 'Views': 'June 16, 2009'}\n",
      "{'Rank': '\"Sugar\"[40]', 'Name': 'Maroon 5', 'Artist': '4.02', 'Upload Date': '', 'Views': 'January 14, 2015'}\n",
      "{'Rank': '\"Baa Baa Black Sheep\"[41]', 'Name': 'Cocomelon - Nursery Rhymes', 'Artist': '4.01', 'Upload Date': '', 'Views': 'June 25, 2018'}\n",
      "{'Rank': '\"Counting Stars\"[42]', 'Name': 'OneRepublic', 'Artist': '4.00', 'Upload Date': '', 'Views': 'May 31, 2013'}\n",
      "{'Rank': '\"Lakdi Ki Kathi\"[43]', 'Name': 'Jingle Toons', 'Artist': '3.98', 'Upload Date': '', 'Views': 'June 14, 2018'}\n",
      "{'Rank': '\"Roar\"[44]', 'Name': 'Katy Perry', 'Artist': '3.98', 'Upload Date': '', 'Views': 'September 5, 2013'}\n",
      "{'Rank': '\"Waka Waka (This Time for Africa)\"[45]', 'Name': 'Shakira', 'Artist': '3.89', 'Upload Date': '', 'Views': 'June 4, 2010'}\n",
      "{'Rank': '\"Sorry\"[46]', 'Name': 'Justin Bieber', 'Artist': '3.78', 'Upload Date': '', 'Views': 'October 22, 2015'}\n",
      "{'Rank': '\"Shree Hanuman Chalisa\"[47]', 'Name': 'T-Series Bhakti Sagar', 'Artist': '3.77', 'Upload Date': '', 'Views': 'May 10, 2011'}\n",
      "{'Rank': '\"Humpty the train on a fruits ride\"[48]', 'Name': 'Kiddiestv Hindi - Nursery Rhymes & Kids Songs', 'Artist': '3.76', 'Upload Date': '', 'Views': 'January 26, 2018'}\n",
      "{'Rank': '\"Thinking Out Loud\"[49]', 'Name': 'Ed Sheeran', 'Artist': '3.75', 'Upload Date': '', 'Views': 'October 7, 2014'}\n",
      "{'Rank': '\"Perfect\"[50]', 'Name': 'Ed Sheeran', 'Artist': '3.70', 'Upload Date': '', 'Views': 'November 9, 2017'}\n",
      "{'Rank': '\"Dark Horse\"[51]', 'Name': 'Katy Perry', 'Artist': '3.70', 'Upload Date': '', 'Views': 'February 20, 2014'}\n",
      "{'Rank': '\"Let Her Go\"[52]', 'Name': 'Passenger', 'Artist': '3.64', 'Upload Date': '', 'Views': 'July 25, 2012'}\n",
      "{'Rank': '\"Faded\"[53]', 'Name': 'Alan Walker', 'Artist': '3.60', 'Upload Date': '', 'Views': 'December 3, 2015'}\n",
      "{'Rank': '\"Girls Like You\"[54]', 'Name': 'Maroon 5', 'Artist': '3.58', 'Upload Date': '', 'Views': 'May 31, 2018'}\n",
      "{'Rank': '\"Lean On\"[55]', 'Name': 'Major Lazer Official', 'Artist': '3.57', 'Upload Date': '', 'Views': 'March 22, 2015'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the most viewed videos\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "# Initialize lists to store data\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Iterate through each row in the table\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    # Extract data from each column if the row contains the expected number of columns\n",
    "    data = row.find_all(\"td\")\n",
    "    if len(data) == 5:  # Check if the row contains the expected number of columns\n",
    "        rank = data[0].text.strip()\n",
    "        name = data[1].text.strip()\n",
    "        artist = data[2].text.strip()\n",
    "        upload_date = data[4].text.strip()\n",
    "        views = data[3].text.strip()\n",
    "\n",
    "        # Append data to respective lists\n",
    "        rank_list.append(rank)\n",
    "        name_list.append(name)\n",
    "        artist_list.append(artist)\n",
    "        upload_date_list.append(upload_date)\n",
    "        views_list.append(views)\n",
    "\n",
    "# Combine the lists into a list of dictionaries\n",
    "videos_data = []\n",
    "for i in range(len(rank_list)):\n",
    "    video = {\n",
    "        \"Rank\": rank_list[i],\n",
    "        \"Name\": name_list[i],\n",
    "        \"Artist\": artist_list[i],\n",
    "        \"Upload Date\": upload_date_list[i],\n",
    "        \"Views\": views_list[i]\n",
    "    }\n",
    "    videos_data.append(video)\n",
    "\n",
    "# Print the scraped data\n",
    "for video in videos_data:\n",
    "    print(video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57827f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No fixtures found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BCCI website\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "# Send a GET request to the BCCI website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_link = soup.find(\"a\", text=\"Fixtures\")[\"href\"]\n",
    "\n",
    "# Construct the URL for the international fixtures page\n",
    "fixtures_url = url + fixtures_link\n",
    "\n",
    "# Send a GET request to the international fixtures page\n",
    "fixtures_response = requests.get(fixtures_url)\n",
    "\n",
    "# Parse the HTML content of the fixtures page\n",
    "fixtures_soup = BeautifulSoup(fixtures_response.content, \"html.parser\")\n",
    "\n",
    "# Find the container with the list of fixtures\n",
    "fixtures_container = fixtures_soup.find(\"div\", class_=\"js-list\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "series_list = []\n",
    "place_list = []\n",
    "date_list = []\n",
    "time_list = []\n",
    "\n",
    "# Check if fixtures_container is not None\n",
    "if fixtures_container:\n",
    "    # Find all fixtures\n",
    "    fixtures = fixtures_container.find_all(\"div\", class_=\"fixture__format-strip\")\n",
    "\n",
    "    # Iterate through each fixture\n",
    "    for fixture in fixtures:\n",
    "        # Extract series, place, date, and time\n",
    "        series = fixture.find(\"div\", class_=\"fixture__format-strip-description\").get_text(strip=True)\n",
    "        place = fixture.find(\"p\", class_=\"fixture__additional-info\").get_text(strip=True)\n",
    "        date = fixture.find(\"span\", class_=\"fixture__date\").get_text(strip=True)\n",
    "        time = fixture.find(\"span\", class_=\"fixture__time\").get_text(strip=True)\n",
    "\n",
    "        # Append data to respective lists\n",
    "        series_list.append(series)\n",
    "        place_list.append(place)\n",
    "        date_list.append(date)\n",
    "        time_list.append(time)\n",
    "\n",
    "    # Print the scraped data\n",
    "    for i in range(len(series_list)):\n",
    "        print(\"Series:\", series_list[i])\n",
    "        print(\"Place:\", place_list[i])\n",
    "        print(\"Date:\", date_list[i])\n",
    "        print(\"Time:\", time_list[i])\n",
    "        print()\n",
    "else:\n",
    "    print(\"No fixtures found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81294d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economy link not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the statisticstimes website\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the economy page based on its URL\n",
    "economy_link = soup.find(\"a\", href=\"/economy\") \n",
    "\n",
    "if economy_link:\n",
    "    # Construct the URL for the economy page\n",
    "    economy_url = url + economy_link[\"href\"]\n",
    "\n",
    "    # Send a GET request to the economy page\n",
    "    economy_response = requests.get(economy_url)\n",
    "\n",
    "    # Parse the HTML content of the economy page\n",
    "    economy_soup = BeautifulSoup(economy_response.content, \"html.parser\")\n",
    "\n",
    "    # Find the link to the State-wise GDP page\n",
    "    gdp_link = economy_soup.find(\"a\", text=\"GDP of Indian states\")[\"href\"]\n",
    "\n",
    "    # Construct the URL for the State-wise GDP page\n",
    "    gdp_url = url + gdp_link\n",
    "\n",
    "    # Send a GET request to the State-wise GDP page\n",
    "    gdp_response = requests.get(gdp_url)\n",
    "\n",
    "    # Parse the HTML content of the State-wise GDP page\n",
    "    gdp_soup = BeautifulSoup(gdp_response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing State-wise GDP data\n",
    "    table = gdp_soup.find(\"table\", {\"id\": \"table_id\"})\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    rank_list = []\n",
    "    state_list = []\n",
    "    gsdp_1819_list = []\n",
    "    gsdp_1920_list = []\n",
    "    share_1819_list = []\n",
    "    gdp_billion_list = []\n",
    "\n",
    "    # Find all rows in the table\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    # Iterate through each row\n",
    "    for row in rows[1:]:  # Skipping the header row\n",
    "        # Extract data from each column\n",
    "        columns = row.find_all(\"td\")\n",
    "        rank = columns[0].text.strip()\n",
    "        state = columns[1].text.strip()\n",
    "        gsdp_1819 = columns[2].text.strip()\n",
    "        gsdp_1920 = columns[3].text.strip()\n",
    "        share_1819 = columns[4].text.strip()\n",
    "        gdp_billion = columns[5].text.strip()\n",
    "\n",
    "        # Append data to respective lists\n",
    "        rank_list.append(rank)\n",
    "        state_list.append(state)\n",
    "        gsdp_1819_list.append(gsdp_1819)\n",
    "        gsdp_1920_list.append(gsdp_1920)\n",
    "        share_1819_list.append(share_1819)\n",
    "        gdp_billion_list.append(gdp_billion)\n",
    "\n",
    "    # Print the scraped data\n",
    "    for i in range(len(rank_list)):\n",
    "        print(\"Rank:\", rank_list[i])\n",
    "        print(\"State:\", state_list[i])\n",
    "        print(\"GSDP(18-19) - Current Prices:\", gsdp_1819_list[i])\n",
    "        print(\"GSDP(19-20) - Current Prices:\", gsdp_1920_list[i])\n",
    "        print(\"Share(18-19):\", share_1819_list[i])\n",
    "        print(\"GDP ($ billion):\", gdp_billion_list[i])\n",
    "        print()\n",
    "else:\n",
    "    print(\"Economy link not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6192f658",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Click on Explore menu\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m explore_menu \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element_by_xpath\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//*[@id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/summary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m explore_menu\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Click on Trending option\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_xpath'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the GitHub website\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "# Click on Explore menu\n",
    "explore_menu = driver.find_element_by_xpath('//*[@id=\"github\"]/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/summary')\n",
    "explore_menu.click()\n",
    "\n",
    "# Click on Trending option\n",
    "trending_option = driver.find_element_by_xpath('//*[@id=\"github\"]/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]/a')\n",
    "trending_option.click()\n",
    "\n",
    "# Wait until the page loads\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"Box-row\")))\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all trending repositories\n",
    "repositories = soup.find_all(\"article\", class_=\"Box-row\")\n",
    "\n",
    "# Extract details for each repository\n",
    "for repo in repositories:\n",
    "    title = repo.find(\"h1\", class_=\"h3 lh-condensed\").text.strip()\n",
    "    description = repo.find(\"p\", class_=\"col-9 color-text-secondary my-1 pr-4\").text.strip()\n",
    "    language = repo.find(\"span\", itemprop=\"programmingLanguage\").text.strip()\n",
    "    contributors = repo.find(\"a\", href=True, text=\"Contributors\")\n",
    "    contributors_count = None\n",
    "    if contributors:\n",
    "        contributors_count = contributors.text.strip()\n",
    "    print(\"Repository Title:\", title)\n",
    "    print(\"Description:\", description)\n",
    "    print(\"Language:\", language)\n",
    "    print(\"Contributors Count:\", contributors_count)\n",
    "    print()\n",
    "    \n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124251c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore menu not found.\n",
      "Trending option not found.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# URL of the GitHub website\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the GitHub website\n",
    "driver.get(url)\n",
    "\n",
    "# Click on Explore menu\n",
    "try:\n",
    "    explore_menu = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"github\"]/body/div[1]/header/div/div[2]/nav/ul/li[4]')))\n",
    "    explore_menu.click()\n",
    "except:\n",
    "    print(\"Explore menu not found.\")\n",
    "\n",
    "# Click on Trending option\n",
    "try:\n",
    "    trending_option = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"github\"]/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]')))\n",
    "    trending_option.click()\n",
    "except:\n",
    "    print(\"Trending option not found.\")\n",
    "\n",
    "# Wait until the page loads\n",
    "time.sleep(5)  # Adding a delay to ensure the page loads completely (you may adjust this value as needed)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all trending repositories\n",
    "repositories = soup.find_all(\"article\", class_=\"Box-row\")\n",
    "\n",
    "# Extract details for each repository\n",
    "for repo in repositories:\n",
    "    title = repo.find(\"h1\", class_=\"h3 lh-condensed\").text.strip()\n",
    "    description = repo.find(\"p\", class_=\"col-9 color-text-secondary my-1 pr-4\").text.strip()\n",
    "    language = repo.find(\"span\", itemprop=\"programmingLanguage\").text.strip()\n",
    "    contributors = repo.find(\"a\", href=True, text=\"Contributors\")\n",
    "    contributors_count = None\n",
    "    if contributors:\n",
    "        contributors_count = contributors.text.strip()\n",
    "    print(\"Repository Title:\", title)\n",
    "    print(\"Description:\", description)\n",
    "    print(\"Language:\", language)\n",
    "    print(\"Contributors Count:\", contributors_count)\n",
    "    print()\n",
    "    \n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "998bad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charts option not found.\n",
      "Hot 100 page link not found.\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=124.0.6367.119)\nStacktrace:\n\tGetHandleVerifier [0x00007FF676031502+60802]\n\t(No symbol) [0x00007FF675FAAC02]\n\t(No symbol) [0x00007FF675E67CE4]\n\t(No symbol) [0x00007FF675E3DFDF]\n\t(No symbol) [0x00007FF675EE1E57]\n\t(No symbol) [0x00007FF675EF98D1]\n\t(No symbol) [0x00007FF675EDA923]\n\t(No symbol) [0x00007FF675EA8FEC]\n\t(No symbol) [0x00007FF675EA9C21]\n\tGetHandleVerifier [0x00007FF67633411D+3217821]\n\tGetHandleVerifier [0x00007FF6763760B7+3488055]\n\tGetHandleVerifier [0x00007FF67636F03F+3459263]\n\tGetHandleVerifier [0x00007FF6760EB846+823494]\n\t(No symbol) [0x00007FF675FB5F9F]\n\t(No symbol) [0x00007FF675FB0EC4]\n\t(No symbol) [0x00007FF675FB1052]\n\t(No symbol) [0x00007FF675FA18A4]\n\tBaseThreadInitThunk [0x00007FF84036257D+29]\n\tRtlUserThreadStart [0x00007FF841D4AA58+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHot 100 page link not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Wait until the page loads\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchart-list__element\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Parse the HTML content of the page\u001b[39;00m\n\u001b[0;32m     32\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py:96\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[0;32m     98\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:84\u001b[0m, in \u001b[0;36mpresence_of_element_located.<locals>._predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    738\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    739\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=124.0.6367.119)\nStacktrace:\n\tGetHandleVerifier [0x00007FF676031502+60802]\n\t(No symbol) [0x00007FF675FAAC02]\n\t(No symbol) [0x00007FF675E67CE4]\n\t(No symbol) [0x00007FF675E3DFDF]\n\t(No symbol) [0x00007FF675EE1E57]\n\t(No symbol) [0x00007FF675EF98D1]\n\t(No symbol) [0x00007FF675EDA923]\n\t(No symbol) [0x00007FF675EA8FEC]\n\t(No symbol) [0x00007FF675EA9C21]\n\tGetHandleVerifier [0x00007FF67633411D+3217821]\n\tGetHandleVerifier [0x00007FF6763760B7+3488055]\n\tGetHandleVerifier [0x00007FF67636F03F+3459263]\n\tGetHandleVerifier [0x00007FF6760EB846+823494]\n\t(No symbol) [0x00007FF675FB5F9F]\n\t(No symbol) [0x00007FF675FB0EC4]\n\t(No symbol) [0x00007FF675FB1052]\n\t(No symbol) [0x00007FF675FA18A4]\n\tBaseThreadInitThunk [0x00007FF84036257D+29]\n\tRtlUserThreadStart [0x00007FF841D4AA58+40]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Billboard website\n",
    "url = \"https://www.billboard.com/\"\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "# Click on the Charts option\n",
    "try:\n",
    "    charts_option = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"menu-charts-desktop\"]/span/span[2]')))\n",
    "    charts_option.click()\n",
    "except:\n",
    "    print(\"Charts option not found.\")\n",
    "\n",
    "# Click on the Hot 100 page link\n",
    "try:\n",
    "    hot_100_link = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main\"]/div[2]/div/div[1]/div[1]/div[2]/a')))\n",
    "    hot_100_link.click()\n",
    "except:\n",
    "    print(\"Hot 100 page link not found.\")\n",
    "\n",
    "# Wait until the page loads\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"chart-list__element\")))\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all chart elements\n",
    "chart_elements = soup.find_all(\"div\", class_=\"chart-list__element\")\n",
    "\n",
    "# Extract details for each song\n",
    "for element in chart_elements:\n",
    "    song_name = element.find(\"span\", class_=\"chart-element__information__song text--truncate color--primary\").text.strip()\n",
    "    artist_name = element.find(\"span\", class_=\"chart-element__information__artist text--truncate color--secondary\").text.strip()\n",
    "    last_week_rank = element.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--last\").text.strip()\n",
    "    peak_rank = element.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--peak\").text.strip()\n",
    "    weeks_on_board = element.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--week\").text.strip()\n",
    "\n",
    "    print(\"Song name:\", song_name)\n",
    "    print(\"Artist name:\", artist_name)\n",
    "    print(\"Last week rank:\", last_week_rank)\n",
    "    print(\"Peak rank:\", peak_rank)\n",
    "    print(\"Weeks on board:\", weeks_on_board)\n",
    "    print()\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d2f2c20",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:  \u001b[38;5;66;03m# Exclude header row\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Extract data from each column\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     book_name \u001b[38;5;241m=\u001b[39m \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     28\u001b[0m     author_name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     29\u001b[0m     volume_sold \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the book details\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "book_names = []\n",
    "author_names = []\n",
    "volumes_sold = []\n",
    "publishers = []\n",
    "genres = []\n",
    "\n",
    "# Iterate through each row in the table\n",
    "for row in table.find_all(\"tr\")[1:]:  # Exclude header row\n",
    "    # Extract data from each column\n",
    "    columns = row.find_all(\"td\")\n",
    "    book_name = columns[1].text.strip()\n",
    "    author_name = columns[2].text.strip()\n",
    "    volume_sold = columns[3].text.strip()\n",
    "    publisher = columns[4].text.strip()\n",
    "    genre = columns[5].text.strip()\n",
    "\n",
    "    # Append data to respective lists\n",
    "    book_names.append(book_name)\n",
    "    author_names.append(author_name)\n",
    "    volumes_sold.append(volume_sold)\n",
    "    publishers.append(publisher)\n",
    "    genres.append(genre)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(book_names)):\n",
    "    print(\"Book name:\", book_names[i])\n",
    "    print(\"Author name:\", author_names[i])\n",
    "    print(\"Volumes sold:\", volumes_sold[i])\n",
    "    print(\"Publisher:\", publishers[i])\n",
    "    print(\"Genre:\", genres[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fd982c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book name: 1\n",
      "Author name: Da Vinci Code,The\n",
      "Volumes sold: Brown, Dan\n",
      "Publisher: 5,094,805\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 2\n",
      "Author name: Harry Potter and the Deathly Hallows\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 4,475,152\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 3\n",
      "Author name: Harry Potter and the Philosopher's Stone\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 4,200,654\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 4\n",
      "Author name: Harry Potter and the Order of the Phoenix\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 4,179,479\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 5\n",
      "Author name: Fifty Shades of Grey\n",
      "Volumes sold: James, E. L.\n",
      "Publisher: 3,758,936\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 6\n",
      "Author name: Harry Potter and the Goblet of Fire\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 3,583,215\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 7\n",
      "Author name: Harry Potter and the Chamber of Secrets\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 3,484,047\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 8\n",
      "Author name: Harry Potter and the Prisoner of Azkaban\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 3,377,906\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 9\n",
      "Author name: Angels and Demons\n",
      "Volumes sold: Brown, Dan\n",
      "Publisher: 3,193,946\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 10\n",
      "Author name: Harry Potter and the Half-blood Prince:Children's Edition\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 2,950,264\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 11\n",
      "Author name: Fifty Shades Darker\n",
      "Volumes sold: James, E. L.\n",
      "Publisher: 2,479,784\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 12\n",
      "Author name: Twilight\n",
      "Volumes sold: Meyer, Stephenie\n",
      "Publisher: 2,315,405\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book name: 13\n",
      "Author name: Girl with the Dragon Tattoo,The:Millennium Trilogy\n",
      "Volumes sold: Larsson, Stieg\n",
      "Publisher: 2,233,570\n",
      "Genre: Quercus\n",
      "\n",
      "Book name: 14\n",
      "Author name: Fifty Shades Freed\n",
      "Volumes sold: James, E. L.\n",
      "Publisher: 2,193,928\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 15\n",
      "Author name: Lost Symbol,The\n",
      "Volumes sold: Brown, Dan\n",
      "Publisher: 2,183,031\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 16\n",
      "Author name: New Moon\n",
      "Volumes sold: Meyer, Stephenie\n",
      "Publisher: 2,152,737\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book name: 17\n",
      "Author name: Deception Point\n",
      "Volumes sold: Brown, Dan\n",
      "Publisher: 2,062,145\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 18\n",
      "Author name: Eclipse\n",
      "Volumes sold: Meyer, Stephenie\n",
      "Publisher: 2,052,876\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book name: 19\n",
      "Author name: Lovely Bones,The\n",
      "Volumes sold: Sebold, Alice\n",
      "Publisher: 2,005,598\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book name: 20\n",
      "Author name: Curious Incident of the Dog in the Night-time,The\n",
      "Volumes sold: Haddon, Mark\n",
      "Publisher: 1,979,552\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 21\n",
      "Author name: Digital Fortress\n",
      "Volumes sold: Brown, Dan\n",
      "Publisher: 1,928,900\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 22\n",
      "Author name: Short History of Nearly Everything,A\n",
      "Volumes sold: Bryson, Bill\n",
      "Publisher: 1,852,919\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 23\n",
      "Author name: Girl Who Played with Fire,The:Millennium Trilogy\n",
      "Volumes sold: Larsson, Stieg\n",
      "Publisher: 1,814,784\n",
      "Genre: Quercus\n",
      "\n",
      "Book name: 24\n",
      "Author name: Breaking Dawn\n",
      "Volumes sold: Meyer, Stephenie\n",
      "Publisher: 1,787,118\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book name: 25\n",
      "Author name: Very Hungry Caterpillar,The:The Very Hungry Caterpillar\n",
      "Volumes sold: Carle, Eric\n",
      "Publisher: 1,783,535\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 26\n",
      "Author name: Gruffalo,The\n",
      "Volumes sold: Donaldson, Julia\n",
      "Publisher: 1,781,269\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book name: 27\n",
      "Author name: Jamie's 30-Minute Meals\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 1,743,266\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 28\n",
      "Author name: Kite Runner,The\n",
      "Volumes sold: Hosseini, Khaled\n",
      "Publisher: 1,629,119\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 29\n",
      "Author name: One Day\n",
      "Volumes sold: Nicholls, David\n",
      "Publisher: 1,616,068\n",
      "Genre: Hodder & Stoughton\n",
      "\n",
      "Book name: 30\n",
      "Author name: Thousand Splendid Suns,A\n",
      "Volumes sold: Hosseini, Khaled\n",
      "Publisher: 1,583,992\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 31\n",
      "Author name: Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\n",
      "Volumes sold: Larsson, Stieg\n",
      "Publisher: 1,555,135\n",
      "Genre: Quercus\n",
      "\n",
      "Book name: 32\n",
      "Author name: Time Traveler's Wife,The\n",
      "Volumes sold: Niffenegger, Audrey\n",
      "Publisher: 1,546,886\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 33\n",
      "Author name: Atonement\n",
      "Volumes sold: McEwan, Ian\n",
      "Publisher: 1,539,428\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 34\n",
      "Author name: Bridget Jones's Diary:A Novel\n",
      "Volumes sold: Fielding, Helen\n",
      "Publisher: 1,508,205\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book name: 35\n",
      "Author name: World According to Clarkson,The\n",
      "Volumes sold: Clarkson, Jeremy\n",
      "Publisher: 1,489,403\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 36\n",
      "Author name: Captain Corelli's Mandolin\n",
      "Volumes sold: Bernieres, Louis de\n",
      "Publisher: 1,352,318\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 37\n",
      "Author name: Sound of Laughter,The\n",
      "Volumes sold: Kay, Peter\n",
      "Publisher: 1,310,207\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 38\n",
      "Author name: Life of Pi\n",
      "Volumes sold: Martel, Yann\n",
      "Publisher: 1,310,176\n",
      "Genre: Canongate\n",
      "\n",
      "Book name: 39\n",
      "Author name: Billy Connolly\n",
      "Volumes sold: Stephenson, Pamela\n",
      "Publisher: 1,231,957\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book name: 40\n",
      "Author name: Child Called It,A\n",
      "Volumes sold: Pelzer, Dave\n",
      "Publisher: 1,217,712\n",
      "Genre: Orion\n",
      "\n",
      "Book name: 41\n",
      "Author name: Gruffalo's Child,The\n",
      "Volumes sold: Donaldson, Julia\n",
      "Publisher: 1,208,711\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book name: 42\n",
      "Author name: Angela's Ashes:A Memoir of a Childhood\n",
      "Volumes sold: McCourt, Frank\n",
      "Publisher: 1,204,058\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book name: 43\n",
      "Author name: Birdsong\n",
      "Volumes sold: Faulks, Sebastian\n",
      "Publisher: 1,184,967\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 44\n",
      "Author name: Northern Lights:His Dark Materials S.\n",
      "Volumes sold: Pullman, Philip\n",
      "Publisher: 1,181,503\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "Book name: 45\n",
      "Author name: Labyrinth\n",
      "Volumes sold: Mosse, Kate\n",
      "Publisher: 1,181,093\n",
      "Genre: Orion\n",
      "\n",
      "Book name: 46\n",
      "Author name: Harry Potter and the Half-blood Prince\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 1,153,181\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 47\n",
      "Author name: Help,The\n",
      "Volumes sold: Stockett, Kathryn\n",
      "Publisher: 1,132,336\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 48\n",
      "Author name: Man and Boy\n",
      "Volumes sold: Parsons, Tony\n",
      "Publisher: 1,130,802\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book name: 49\n",
      "Author name: Memoirs of a Geisha\n",
      "Volumes sold: Golden, Arthur\n",
      "Publisher: 1,126,337\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 50\n",
      "Author name: No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\n",
      "Volumes sold: McCall Smith, Alexander\n",
      "Publisher: 1,115,549\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book name: 51\n",
      "Author name: Island,The\n",
      "Volumes sold: Hislop, Victoria\n",
      "Publisher: 1,108,328\n",
      "Genre: Headline\n",
      "\n",
      "Book name: 52\n",
      "Author name: PS, I Love You\n",
      "Volumes sold: Ahern, Cecelia\n",
      "Publisher: 1,107,379\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book name: 53\n",
      "Author name: You are What You Eat:The Plan That Will Change Your Life\n",
      "Volumes sold: McKeith, Gillian\n",
      "Publisher: 1,104,403\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 54\n",
      "Author name: Shadow of the Wind,The\n",
      "Volumes sold: Zafon, Carlos Ruiz\n",
      "Publisher: 1,092,349\n",
      "Genre: Orion\n",
      "\n",
      "Book name: 55\n",
      "Author name: Tales of Beedle the Bard,The\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 1,090,847\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book name: 56\n",
      "Author name: Broker,The\n",
      "Volumes sold: Grisham, John\n",
      "Publisher: 1,087,262\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 57\n",
      "Author name: Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\n",
      "Volumes sold: Atkins, Robert C.\n",
      "Publisher: 1,054,196\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 58\n",
      "Author name: Subtle Knife,The:His Dark Materials S.\n",
      "Volumes sold: Pullman, Philip\n",
      "Publisher: 1,037,160\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "Book name: 59\n",
      "Author name: Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation\n",
      "Volumes sold: Truss, Lynne\n",
      "Publisher: 1,023,688\n",
      "Genre: Profile Books Group\n",
      "\n",
      "Book name: 60\n",
      "Author name: Delia's How to Cook:(Bk.1)\n",
      "Volumes sold: Smith, Delia\n",
      "Publisher: 1,015,956\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 61\n",
      "Author name: Chocolat\n",
      "Volumes sold: Harris, Joanne\n",
      "Publisher: 1,009,873\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 62\n",
      "Author name: Boy in the Striped Pyjamas,The\n",
      "Volumes sold: Boyne, John\n",
      "Publisher: 1,004,414\n",
      "Genre: Random House Childrens Books G\n",
      "\n",
      "Book name: 63\n",
      "Author name: My Sister's Keeper\n",
      "Volumes sold: Picoult, Jodi\n",
      "Publisher: 1,003,780\n",
      "Genre: Hodder & Stoughton\n",
      "\n",
      "Book name: 64\n",
      "Author name: Amber Spyglass,The:His Dark Materials S.\n",
      "Volumes sold: Pullman, Philip\n",
      "Publisher: 1,002,314\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "Book name: 65\n",
      "Author name: To Kill a Mockingbird\n",
      "Volumes sold: Lee, Harper\n",
      "Publisher: 998,213\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 66\n",
      "Author name: Men are from Mars, Women are from Venus:A Practical Guide for Improvin\n",
      "Volumes sold: Gray, John\n",
      "Publisher: 992,846\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book name: 67\n",
      "Author name: Dear Fatty\n",
      "Volumes sold: French, Dawn\n",
      "Publisher: 986,753\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 68\n",
      "Author name: Short History of Tractors in Ukrainian,A\n",
      "Volumes sold: Lewycka, Marina\n",
      "Publisher: 986,115\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 69\n",
      "Author name: Hannibal\n",
      "Volumes sold: Harris, Thomas\n",
      "Publisher: 970,509\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 70\n",
      "Author name: Lord of the Rings,The\n",
      "Volumes sold: Tolkien, J. R. R.\n",
      "Publisher: 967,466\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book name: 71\n",
      "Author name: Stupid White Men:...and Other Sorry Excuses for the State of the Natio\n",
      "Volumes sold: Moore, Michael\n",
      "Publisher: 963,353\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 72\n",
      "Author name: Interpretation of Murder,The\n",
      "Volumes sold: Rubenfeld, Jed\n",
      "Publisher: 962,515\n",
      "Genre: Headline\n",
      "\n",
      "Book name: 73\n",
      "Author name: Sharon Osbourne Extreme:My Autobiography\n",
      "Volumes sold: Osbourne, Sharon\n",
      "Publisher: 959,496\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book name: 74\n",
      "Author name: Alchemist,The:A Fable About Following Your Dream\n",
      "Volumes sold: Coelho, Paulo\n",
      "Publisher: 956,114\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book name: 75\n",
      "Author name: At My Mother's Knee ...:and Other Low Joints\n",
      "Volumes sold: O'Grady, Paul\n",
      "Publisher: 945,640\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 76\n",
      "Author name: Notes from a Small Island\n",
      "Volumes sold: Bryson, Bill\n",
      "Publisher: 931,312\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 77\n",
      "Author name: Return of the Naked Chef,The\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 925,425\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 78\n",
      "Author name: Bridget Jones: The Edge of Reason\n",
      "Volumes sold: Fielding, Helen\n",
      "Publisher: 924,695\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book name: 79\n",
      "Author name: Jamie's Italy\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 906,968\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 80\n",
      "Author name: I Can Make You Thin\n",
      "Volumes sold: McKenna, Paul\n",
      "Publisher: 905,086\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 81\n",
      "Author name: Down Under\n",
      "Volumes sold: Bryson, Bill\n",
      "Publisher: 890,847\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 82\n",
      "Author name: Summons,The\n",
      "Volumes sold: Grisham, John\n",
      "Publisher: 869,671\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 83\n",
      "Author name: Small Island\n",
      "Volumes sold: Levy, Andrea\n",
      "Publisher: 869,659\n",
      "Genre: Headline\n",
      "\n",
      "Book name: 84\n",
      "Author name: Nigella Express\n",
      "Volumes sold: Lawson, Nigella\n",
      "Publisher: 862,602\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 85\n",
      "Author name: Brick Lane\n",
      "Volumes sold: Ali, Monica\n",
      "Publisher: 856,540\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 86\n",
      "Author name: Memory Keeper's Daughter,The\n",
      "Volumes sold: Edwards, Kim\n",
      "Publisher: 845,858\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 87\n",
      "Author name: Room on the Broom\n",
      "Volumes sold: Donaldson, Julia\n",
      "Publisher: 842,535\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book name: 88\n",
      "Author name: About a Boy\n",
      "Volumes sold: Hornby, Nick\n",
      "Publisher: 828,215\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 89\n",
      "Author name: My Booky Wook\n",
      "Volumes sold: Brand, Russell\n",
      "Publisher: 820,563\n",
      "Genre: Hodder & Stoughton\n",
      "\n",
      "Book name: 90\n",
      "Author name: God Delusion,The\n",
      "Volumes sold: Dawkins, Richard\n",
      "Publisher: 816,907\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 91\n",
      "Author name: \"Beano\" Annual,The\n",
      "Volumes sold: 0\n",
      "Publisher: 816,585\n",
      "Genre: D.C. Thomson\n",
      "\n",
      "Book name: 92\n",
      "Author name: White Teeth\n",
      "Volumes sold: Smith, Zadie\n",
      "Publisher: 815,586\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 93\n",
      "Author name: House at Riverton,The\n",
      "Volumes sold: Morton, Kate\n",
      "Publisher: 814,370\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book name: 94\n",
      "Author name: Book Thief,The\n",
      "Volumes sold: Zusak, Markus\n",
      "Publisher: 809,641\n",
      "Genre: Transworld\n",
      "\n",
      "Book name: 95\n",
      "Author name: Nights of Rain and Stars\n",
      "Volumes sold: Binchy, Maeve\n",
      "Publisher: 808,900\n",
      "Genre: Orion\n",
      "\n",
      "Book name: 96\n",
      "Author name: Ghost,The\n",
      "Volumes sold: Harris, Robert\n",
      "Publisher: 807,311\n",
      "Genre: Random House\n",
      "\n",
      "Book name: 97\n",
      "Author name: Happy Days with the Naked Chef\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 794,201\n",
      "Genre: Penguin\n",
      "\n",
      "Book name: 98\n",
      "Author name: Hunger Games,The:Hunger Games Trilogy\n",
      "Volumes sold: Collins, Suzanne\n",
      "Publisher: 792,187\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "Book name: 99\n",
      "Author name: Lost Boy,The:A Foster Child's Search for the Love of a Family\n",
      "Volumes sold: Pelzer, Dave\n",
      "Publisher: 791,507\n",
      "Genre: Orion\n",
      "\n",
      "Book name: 100\n",
      "Author name: Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 791,095\n",
      "Genre: Penguin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the book details\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "book_names = []\n",
    "author_names = []\n",
    "volumes_sold = []\n",
    "publishers = []\n",
    "genres = []\n",
    "\n",
    "# Iterate through each row in the table\n",
    "for row in table.find_all(\"tr\")[1:]:  # Exclude header row\n",
    "    # Extract data from each column if present\n",
    "    columns = row.find_all(\"td\")\n",
    "    if len(columns) >= 5:  # Ensure columns are present\n",
    "        book_name = columns[0].text.strip()\n",
    "        author_name = columns[1].text.strip()\n",
    "        volume_sold = columns[2].text.strip()\n",
    "        publisher = columns[3].text.strip()\n",
    "        genre = columns[4].text.strip()\n",
    "\n",
    "        # Append data to respective lists\n",
    "        book_names.append(book_name)\n",
    "        author_names.append(author_name)\n",
    "        volumes_sold.append(volume_sold)\n",
    "        publishers.append(publisher)\n",
    "        genres.append(genre)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(book_names)):\n",
    "    print(\"Book name:\", book_names[i])\n",
    "    print(\"Author name:\", author_names[i])\n",
    "    print(\"Volumes sold:\", volumes_sold[i])\n",
    "    print(\"Publisher:\", publishers[i])\n",
    "    print(\"Genre:\", genres[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a8e1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all TV series items\n",
    "series_items = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "names = []\n",
    "year_spans = []\n",
    "genres = []\n",
    "runtimes = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "# Iterate through each series item\n",
    "for item in series_items:\n",
    "    # Extract data from each item\n",
    "    name = item.find(\"h3\").a.text.strip()\n",
    "    year_span = item.find(\"span\", class_=\"lister-item-year\").text.strip(\"()\")\n",
    "    genre = item.find(\"span\", class_=\"genre\").text.strip()\n",
    "    runtime = item.find(\"span\", class_=\"runtime\").text.strip()\n",
    "    rating = item.find(\"span\", class_=\"ipl-rating-star__rating\").text.strip()\n",
    "    vote = item.find(\"span\", attrs={\"name\": \"nv\"})[\"data-value\"]\n",
    "\n",
    "    # Append data to respective lists\n",
    "    names.append(name)\n",
    "    year_spans.append(year_span)\n",
    "    genres.append(genre)\n",
    "    runtimes.append(runtime)\n",
    "    ratings.append(rating)\n",
    "    votes.append(vote)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(names)):\n",
    "    print(\"Name:\", names[i])\n",
    "    print(\"Year span:\", year_spans[i])\n",
    "    print(\"Genre:\", genres[i])\n",
    "    print(\"Run time:\", runtimes[i])\n",
    "    print(\"Rating:\", ratings[i])\n",
    "    print(\"Votes:\", votes[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6398c86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to 'Show All Dataset' page not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the home page\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "# Send a GET request to the home page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the \"Show All Dataset\" page\n",
    "all_dataset_link = soup.find(\"a\", href=\"datasets.php\")\n",
    "\n",
    "if all_dataset_link:\n",
    "    # Construct the URL for the \"Show All Dataset\" page\n",
    "    all_dataset_url = url + all_dataset_link[\"href\"]\n",
    "\n",
    "    # Send a GET request to the \"Show All Dataset\" page\n",
    "    response = requests.get(all_dataset_url)\n",
    "\n",
    "    # Parse the HTML content of the \"Show All Dataset\" page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all dataset entries\n",
    "    dataset_entries = soup.find_all(\"tr\", bgcolor=\"#ffffff\")\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    dataset_names = []\n",
    "    data_types = []\n",
    "    tasks = []\n",
    "    attribute_types = []\n",
    "    num_instances = []\n",
    "    num_attributes = []\n",
    "    years = []\n",
    "\n",
    "    # Iterate through each dataset entry\n",
    "    for entry in dataset_entries:\n",
    "        # Extract data from each entry\n",
    "        cells = entry.find_all(\"td\")\n",
    "        dataset_names.append(cells[0].text.strip())\n",
    "        data_types.append(cells[1].text.strip())\n",
    "        tasks.append(cells[2].text.strip())\n",
    "        attribute_types.append(cells[3].text.strip())\n",
    "        num_instances.append(cells[4].text.strip())\n",
    "        num_attributes.append(cells[5].text.strip())\n",
    "        years.append(cells[6].text.strip())\n",
    "\n",
    "    # Print the scraped data\n",
    "    for i in range(len(dataset_names)):\n",
    "        print(\"Dataset name:\", dataset_names[i])\n",
    "        print(\"Data type:\", data_types[i])\n",
    "        print(\"Task:\", tasks[i])\n",
    "        print(\"Attribute type:\", attribute_types[i])\n",
    "        print(\"No of instances:\", num_instances[i])\n",
    "        print(\"No of attributes:\", num_attributes[i])\n",
    "        print(\"Year:\", years[i])\n",
    "        print()\n",
    "\n",
    "else:\n",
    "    print(\"Link to 'Show All Dataset' page not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e0629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
