{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42043a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29dedf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb's Top Rated 100 Indian Movies:\n",
      "                                 Name Rating  Year\n",
      "0                     Ship of Theseus   None  2012\n",
      "1                              Iruvar   None  1997\n",
      "2                     Kaagaz Ke Phool   None  1959\n",
      "3   Lagaan: Once Upon a Time in India   None  2001\n",
      "4                     Pather Panchali   None  1955\n",
      "..                                ...    ...   ...\n",
      "95                        Apur Sansar   None  1959\n",
      "96                        Kanchivaram   None  2008\n",
      "97                    Monsoon Wedding   None  2001\n",
      "98                              Black   None  2005\n",
      "99                            Deewaar   None  1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape IMDb's Top 100 Indian movies\n",
    "def scrape_imdb_top_100_indian_movies(url):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract movie details\n",
    "    movies_data = []\n",
    "    for movie in soup.select('.lister-item-content'):\n",
    "        # Check if rating is available\n",
    "        rating_tag = movie.select_one('.ratings-imdb-rating strong')\n",
    "        rating = float(rating_tag.get_text(strip=True)) if rating_tag else None\n",
    "        \n",
    "        # Check if year is available\n",
    "        year_tag = movie.select_one('.lister-item-year')\n",
    "        year_text = year_tag.get_text(strip=True).strip('()') if year_tag else None\n",
    "        year = int(''.join(filter(str.isdigit, year_text))) if year_text else None\n",
    "        \n",
    "        name = movie.select_one('.lister-item-header a').get_text(strip=True)\n",
    "        \n",
    "        movies_data.append({'Name': name, 'Rating': rating, 'Year': year})\n",
    "\n",
    "    # Create DataFrame\n",
    "    movies_df = pd.DataFrame(movies_data)\n",
    "\n",
    "    return movies_df\n",
    "\n",
    "# Test the function\n",
    "imdb_url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "movies_df = scrape_imdb_top_100_indian_movies(imdb_url)\n",
    "print(\"IMDb's Top Rated 100 Indian Movies:\")\n",
    "print(movies_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8972737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Product Details from Peachmode:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape product name, price, and discounts from Peachmode\n",
    "def scrape_peachmode_products(url):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract product details\n",
    "    products_data = []\n",
    "    for product in soup.select('.product-grid-item'):\n",
    "        name = product.select_one('.name').get_text(strip=True)\n",
    "        price = product.select_one('.price').get_text(strip=True)\n",
    "        discount = product.select_one('.discount').get_text(strip=True)\n",
    "        products_data.append({'Product Name': name, 'Price': price, 'Discount': discount})\n",
    "\n",
    "    # Create DataFrame\n",
    "    products_df = pd.DataFrame(products_data)\n",
    "\n",
    "    return products_df\n",
    "\n",
    "# Test the function\n",
    "peachmode_url = \"https://peachmode.com/search?q=bags\"\n",
    "products_df = scrape_peachmode_products(peachmode_url)\n",
    "print(\"Scraped Product Details from Peachmode:\")\n",
    "print(products_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2086de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape Top 10 ODI teams in men’s cricket along with matches, points, and rating\n",
    "def scrape_odi_teams(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    teams_data = []\n",
    "    for team in soup.select('.table-body'):\n",
    "        name = team.select_one('.table-body__cell.rankings-table__team').get_text(strip=True)\n",
    "        matches = team.select_one('.table-body__cell.u-center-text').get_text(strip=True)\n",
    "        points = team.select('td')[3].get_text(strip=True)\n",
    "        rating = team.select_one('.table-body__cell.u-text-right.rating').get_text(strip=True)\n",
    "        teams_data.append({'Team': name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "        if len(teams_data) >= 10:\n",
    "            break\n",
    "\n",
    "    teams_df = pd.DataFrame(teams_data)\n",
    "    return teams_df\n",
    "\n",
    "# Function to scrape Top 10 ODI Batsmen along with team and rating\n",
    "def scrape_odi_batsmen(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    batsmen_data = []\n",
    "    for player in soup.select('.table-body'):\n",
    "        name = player.select_one('.table-body__cell.rankings-table__name a').get_text(strip=True)\n",
    "        team = player.select_one('.table-body__cell.rankings-table__team').get_text(strip=True)\n",
    "        rating = player.select_one('.table-body__cell.u-text-right.rating').get_text(strip=True)\n",
    "        batsmen_data.append({'Batsman': name, 'Team': team, 'Rating': rating})\n",
    "        if len(batsmen_data) >= 10:\n",
    "            break\n",
    "\n",
    "    batsmen_df = pd.DataFrame(batsmen_data)\n",
    "    return batsmen_df\n",
    "\n",
    "# Function to scrape Top 10 ODI bowlers along with team and rating\n",
    "def scrape_odi_bowlers(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    bowlers_data = []\n",
    "    for player in soup.select('.table-body'):\n",
    "        name = player.select_one('.table-body__cell.rankings-table__name a').get_text(strip=True)\n",
    "        team = player.select_one('.table-body__cell.rankings-table__team').get_text(strip=True)\n",
    "        rating = player.select_one('.table-body__cell.u-text-right.rating').get_text(strip=True)\n",
    "        bowlers_data.append({'Bowler': name, 'Team': team, 'Rating': rating})\n",
    "        if len(bowlers_data) >= 10:\n",
    "            break\n",
    "\n",
    "    bowlers_df = pd.DataFrame(bowlers_data)\n",
    "    return bowlers_df\n",
    "\n",
    "# Test the functions\n",
    "teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(scrape_odi_teams(teams_url))\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(scrape_odi_batsmen(batsmen_url))\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(scrape_odi_bowlers(bowlers_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ba951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def scrape_patreon_posts(url):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract post details\n",
    "    posts_data = []\n",
    "    for post in soup.select('.postItem'):\n",
    "        # Extract heading\n",
    "        heading = post.select_one('.postItem__title').get_text(strip=True)\n",
    "\n",
    "        # Extract date\n",
    "        date = post.select_one('.postItem__date').get_text(strip=True)\n",
    "\n",
    "        # Extract content\n",
    "        content = post.select_one('.postItem__content').get_text(strip=True)\n",
    "\n",
    "        # Extract YouTube video link\n",
    "        youtube_link = post.select_one('.postItem__media__content a[href*=\"youtube.com\"]')\n",
    "        youtube_video_likes = None\n",
    "        if youtube_link:\n",
    "            youtube_url = youtube_link['href']\n",
    "            youtube_response = requests.get(youtube_url)\n",
    "            youtube_soup = BeautifulSoup(youtube_response.text, 'html.parser')\n",
    "            youtube_video_likes_element = youtube_soup.select_one('.like-button-renderer-like-button-unclicked span')\n",
    "            if youtube_video_likes_element:\n",
    "                youtube_video_likes = youtube_video_likes_element.text.strip()\n",
    "\n",
    "        # Append data to list\n",
    "        posts_data.append({'Heading': heading, 'Date': date, 'Content': content, 'YouTube Video Likes': youtube_video_likes})\n",
    "\n",
    "    # Create DataFrame\n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "\n",
    "    return posts_df\n",
    "\n",
    "# Test the function\n",
    "patreon_url = \"https://www.patreon.com/coreyms\"\n",
    "posts_df = scrape_patreon_posts(patreon_url)\n",
    "print(posts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a06a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping house details for Indira-Nagar:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Scraping house details for Jayanagar:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Scraping house details for Rajaji-Nagar:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_house_details(locality):\n",
    "    # Base URL for searching house listings\n",
    "    base_url = f\"https://www.nobroker.in/property/sale/bangalore/{locality}\"\n",
    "\n",
    "    # Send request to the URL\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract house details\n",
    "    houses_data = []\n",
    "    for house in soup.select('.card'):\n",
    "        title = house.select_one('.card-title').get_text(strip=True)\n",
    "        location = house.select_one('.card-title ~ .card-subtitle').get_text(strip=True)\n",
    "        area = house.select_one('.card-text-area').get_text(strip=True)\n",
    "        emi = house.select_one('.card-text-emidiv').get_text(strip=True)\n",
    "        price = house.select_one('.card-price').get_text(strip=True)\n",
    "        houses_data.append({'Title': title, 'Location': location, 'Area': area, 'EMI': emi, 'Price': price})\n",
    "\n",
    "    # Create DataFrame\n",
    "    houses_df = pd.DataFrame(houses_data)\n",
    "\n",
    "    return houses_df\n",
    "\n",
    "# Specify localities\n",
    "localities = ['Indira-Nagar', 'Jayanagar', 'Rajaji-Nagar']\n",
    "\n",
    "# Scrape house details for each locality\n",
    "for locality in localities:\n",
    "    print(f\"Scraping house details for {locality}:\")\n",
    "    houses_df = scrape_house_details(locality)\n",
    "    print(houses_df)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d748f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Product Details:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def scrape_product_details(url):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract product details\n",
    "    products_data = []\n",
    "    for product in soup.select('.productCardWrapper')[:10]:  # Limit to first 10 products\n",
    "        name = product.select_one('.productCardDetail').get_text(strip=True)\n",
    "        price = product.select_one('.productPrice').get_text(strip=True)\n",
    "        image_url = product.select_one('.productCardImg')['src']\n",
    "        products_data.append({'Product Name': name, 'Price': price, 'Image URL': image_url})\n",
    "\n",
    "    # Create DataFrame\n",
    "    products_df = pd.DataFrame(products_data)\n",
    "\n",
    "    return products_df\n",
    "\n",
    "# URL of the page containing the list of bestsellers\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "\n",
    "# Scrape product details\n",
    "products_df = scrape_product_details(url)\n",
    "print(\"Scraped Product Details:\")\n",
    "print(products_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e3644a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped CNBC World News Details:\n",
      "                                              Heading  Date News Link\n",
      "0   Stocks making the biggest moves midday: GameSt...  None      None\n",
      "1   A majority of investors believe a stock market...  None      None\n",
      "2   S&P 500 rises as it tries to snap 3-day slide,...  None      None\n",
      "3   Treasury yields slide as investors weigh econo...  None      None\n",
      "4   JPMorgan says oil can rise to nearly $100 a ba...  None      None\n",
      "5   Cathie Wood's ARK ETF is forming a bottoming p...  None      None\n",
      "6   Citi says buy this chip stock ahead of its AI ...  None      None\n",
      "7   Chip stocks are losing their mojo. How to trad...  None      None\n",
      "8   These three stocks are on the verge of forming...  None      None\n",
      "9   Russia claimed the West, Kyiv ordered the Mosc...  None      None\n",
      "10  Russia says it's 'hard to believe' Islamic Sta...  None      None\n",
      "11  Russia's intelligence chief claims U.S., U.K. ...  None      None\n",
      "12  Putin expected to use deadly Moscow attack to ...  None      None\n",
      "13  Moscow terror attack suspects appear in court ...  None      None\n",
      "14  Nikolaj Coster-Waldau on the UN’s 'Weather Kid...  None      None\n",
      "15  Hopes for buried hydrogen’s clean energy poten...  None      None\n",
      "16  Earth on the brink of critical warming thresho...  None      None\n",
      "17  Modular construction, seen as a solution to ho...  None      None\n",
      "18  Paris has hiked parking charges on SUVs. Now c...  None      None\n",
      "19  'Quiet luxury' was once all about fashion — bu...  None      None\n",
      "20  Passport-free travel in Singapore is here — bu...  None      None\n",
      "21  Japan launches bullet train service to a regio...  None      None\n",
      "22  A new warning to Bali tourists: 'Stay away fro...  None      None\n",
      "23  Here are the most powerful passports for 2024 ...  None      None\n",
      "24  The top misconception about 'hustling' to get ...  None      None\n",
      "25  Non-members will no longer be able to buy Cost...  None      None\n",
      "26  The income a family of 4 needs to live comfort...  None      None\n",
      "27  These are the 10 top-paying U.S. cities where ...  None      None\n",
      "28  The ultimate guide to earning passive income o...  None      None\n",
      "29                  How China's property bubble burst  None      None\n",
      "30                  What is the World Economic Forum?  None      None\n",
      "31  The Quad is going beyond military exercises — ...  None      None\n",
      "32     Why a coup in Guinea was felt around the world  None      None\n",
      "33  COP26 had big ambitions — here's why it fell s...  None      None\n"
     ]
    }
   ],
   "source": [
    "def scrape_cnbc_world_news(url):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract news details\n",
    "    news_data = []\n",
    "    for article in soup.select('.Card-title'):\n",
    "        # Extract heading\n",
    "        heading = article.get_text(strip=True)\n",
    "\n",
    "        # Extract date\n",
    "        try:\n",
    "            date_tag = article.find_previous(class_='Card-meta-time')\n",
    "            date = date_tag.get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            date = None\n",
    "\n",
    "        # Extract news link\n",
    "        news_link_tag = article.find_parent('a')\n",
    "        news_link = news_link_tag['href'] if news_link_tag else None\n",
    "\n",
    "        news_data.append({'Heading': heading, 'Date': date, 'News Link': news_link})\n",
    "\n",
    "    # Create DataFrame\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "\n",
    "    return news_df\n",
    "\n",
    "# URL of the CNBC World news page\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Scrape news details\n",
    "news_df = scrape_cnbc_world_news(url)\n",
    "print(\"Scraped CNBC World News Details:\")\n",
    "print(news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ae7780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Most Downloaded Articles Details:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def scrape_downloaded_articles(url):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract article details\n",
    "    articles_data = []\n",
    "    for article in soup.select('.view-content .list-article'):\n",
    "        # Extract paper title\n",
    "        title = article.select_one('.list-article-title').get_text(strip=True)\n",
    "\n",
    "        # Extract publication date\n",
    "        date = article.select_one('.list-article-date').get_text(strip=True)\n",
    "\n",
    "        # Extract author\n",
    "        author = article.select_one('.list-article-author').get_text(strip=True)\n",
    "\n",
    "        articles_data.append({'Paper Title': title, 'Date': date, 'Author': author})\n",
    "\n",
    "    # Create DataFrame\n",
    "    articles_df = pd.DataFrame(articles_data)\n",
    "\n",
    "    return articles_df\n",
    "\n",
    "# URL of the most downloaded articles page\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/\"\n",
    "\n",
    "# Scrape article details\n",
    "articles_df = scrape_downloaded_articles(url)\n",
    "print(\"Scraped Most Downloaded Articles Details:\")\n",
    "print(articles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2762a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
